{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e443cb-6a17-4df4-925a-09efd6ef5edb",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "#import imutils\n",
    "#from imutils.contours import sort_contours\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sewar.full_ref import uqi, vifp, psnrb\n",
    "\n",
    "import string    \n",
    "import re\n",
    "\n",
    "import time \n",
    "import base64\n",
    "import pendulum\n",
    "from datetime import datetime\n",
    "\n",
    "import os.path\n",
    "import os\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e46034c1-3a87-4bda-9f28-ff789f67e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import re\n",
    "import selenium\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import imutils\n",
    "from imutils.contours import sort_contours\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a590f7c-9f66-4dca-bb92-a4e5756b63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_COMMON_PATH = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "138fff99-7f84-40c2-980f-ca4f99403060",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FILE_COMMON_PATH + \"lib_common.ipynb\"\n",
    "%run $f\n",
    "\n",
    "f = FILE_COMMON_PATH + \"lib_db.ipynb\"\n",
    "%run $f\n",
    "\n",
    "f = FILE_COMMON_PATH + \"config.ipynb\"\n",
    "%run $f\n",
    "\n",
    "f = FILE_COMMON_PATH + \"lib_crawl.ipynb\"\n",
    "%run $f\n",
    "\n",
    "f = FILE_COMMON_PATH + \"lib_ftp.ipynb\"\n",
    "%run $f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85022ae8-6f05-4ac1-bd64-5d5c01d53b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictCaptcha:\n",
    "    def __init__(self, fmodel = CAPTCHA_TAX_MODEL):\n",
    "        self.fn_model = load_model(fmodel)\n",
    "        \n",
    "    @staticmethod\n",
    "    def remove_noise(fimg):\n",
    "        def convert_jpg_(fimg):\n",
    "\n",
    "            png_image = Image.open(fimg) if type(fimg) is str else fimg\n",
    "\n",
    "            jpg_image = Image.new(\"RGB\", png_image.size, (255, 255, 255))  # Creating a white background\n",
    "            jpg_image.paste(png_image, (0, 0), png_image)\n",
    "            jpeg_array = np.array(jpg_image)\n",
    "\n",
    "            return jpeg_array\n",
    "\n",
    "        image = cv2.imread(fimg, cv2.IMREAD_UNCHANGED) if type(fimg) is str and '.jpg' in fimg else \\\n",
    "                        convert_jpg_(fimg)\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        dilated_img = cv2.dilate(image, kernel, iterations=1)\n",
    "\n",
    "        return dilated_img\n",
    "\n",
    "    @staticmethod\n",
    "    def chop_multi_versions(img, num_chars, chop_wd = [0, -2, -4, 2, 4], chop_width=[0]):\n",
    "        def find_first_position_(img):\n",
    "            arr_pos = []\n",
    "\n",
    "            for r in range(0, img.shape[0]):\n",
    "                fwhite = None\n",
    "                for c in range(0, img.shape[1]):\n",
    "                    s = sum(img[r][c])\n",
    "                    fwhite = c if s == 765 else fwhite            \n",
    "                    if fwhite is not None and (s != 765):  \n",
    "                        arr_pos.append(c) if c < img.shape[1] - 3 else None\n",
    "                        break\n",
    "            return min(arr_pos)\n",
    "\n",
    "        def find_last_position_(img):\n",
    "            arr_pos = []\n",
    "\n",
    "            for r in range(0, img.shape[0]):\n",
    "                fwhite = None\n",
    "                for c in range(0, img.shape[1]):\n",
    "                    c = img.shape[1] - c - 1\n",
    "                    s = sum(img[r][c])            \n",
    "                    fwhite = c if s == 765 else fwhite\n",
    "                    if fwhite is not None and (s != 765):\n",
    "                        arr_pos.append(c) if c > 5 else None\n",
    "                        break\n",
    "\n",
    "            return max(arr_pos)\n",
    "\n",
    "        def chop_first_last_(img, first_chop, last_chop):\n",
    "            return np.array([img[i][first_chop:last_chop] for i in range(0, img.shape[0])])\n",
    "\n",
    "        def chop_equal_(img, num_chars):\n",
    "            equal_size = int(img.shape[1]/num_chars)\n",
    "            return [img[0:img.shape[0], i:i+equal_size,:] for i in range(0, equal_size*num_chars, equal_size)]\n",
    "\n",
    "\n",
    "        def chop_multi_(img, num_chars, chop_wd = [0, -2, -4, 2, 4], chop_width=[0]):\n",
    "            equal_size = int(img.shape[1]/num_chars)\n",
    "            chop_multi = [[] for i in range(0, num_chars)]\n",
    "            chop_size = [(x, x, w) for x in chop_wd for w in chop_width]        \n",
    "            for i in range(0, num_chars):\n",
    "                for s in chop_size:\n",
    "                    ri = i*equal_size\n",
    "                    start, end = max(0, ri + s[0]), min(img.shape[1], ri + equal_size + s[1] + s[2])\n",
    "\n",
    "                    t_img = img[0:img.shape[0], start:end,:]\n",
    "\n",
    "                    # padding if width less than equal width\n",
    "                    num_padded = (equal_size + max(chop_width)) - (end - start)\n",
    "                    num_left = int(num_padded / 2)\n",
    "                    num_right = num_padded - num_left\n",
    "\n",
    "                    padded_left = np.full((num_left, 3), 255) if num_padded > 0 else None\n",
    "                    padded_right = np.full((num_right, 3), 255) if num_padded > 0 else None\n",
    "                    t_img = np.array([np.concatenate((padded_left, t, padded_right), axis=0) \n",
    "                                              for t in t_img]) if num_padded > 0 else t_img\n",
    "\n",
    "                    chop_multi[i].append(t_img)\n",
    "\n",
    "            return chop_multi\n",
    "\n",
    "        def padding_multi_(chop_multi, padded = np.full((2, 3), 255)):\n",
    "            padded_multi = [[] for i in range(len(chop_multi))]\n",
    "            for i in range(len(chop_multi)):\n",
    "                for j in range(len(chop_multi[i])):\n",
    "                    img = chop_multi[i][j]\n",
    "                    t_img = np.array([np.concatenate((padded, t, padded), axis=0) for t in img])\n",
    "                    padded_multi[i].append(t_img.astype(np.uint8))\n",
    "\n",
    "            return padded_multi\n",
    "\n",
    "\n",
    "        first_chop = find_first_position_(img)\n",
    "        last_chop = find_last_position_(img)\n",
    "        chop_image = chop_first_last_(img, first_chop, last_chop)    \n",
    "        chop_list = chop_multi_(chop_image, num_chars, chop_wd, chop_width)    \n",
    "        padding_list = padding_multi_(chop_list)\n",
    "\n",
    "        return padding_list\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_contours_all(img_list):\n",
    "        def extract_contours_(img):\n",
    "            img = imutils.resize(img, width=32, height=32)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "            edged = cv2.Canny(blurred, img.shape[0], img.shape[1])\n",
    "            # edged = cv2.Canny(img, img.shape[0], img.shape[1])\n",
    "            cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cnts = imutils.grab_contours(cnts)\n",
    "            # cnts_sorted = sorted(cnts, key=lambda c: cv2.boundingRect(c)[0])\n",
    "            cnts_sorted = cnts\n",
    "\n",
    "            chars = []\n",
    "            for i, c in enumerate(cnts_sorted):\n",
    "\n",
    "                # compute the bounding box of the contour\n",
    "                (x, y, w, h) = cv2.boundingRect(c)            \n",
    "\n",
    "                if w >= 10 and h >= 10:\n",
    "                    roi = edged[y:y + h, x:x + w]\n",
    "                    thresh = cv2.threshold(roi, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "                    (tH, tW) = thresh.shape\n",
    "\n",
    "                    # if the width is greater than the height, resize along the width dimension\n",
    "                    rsw, rsh = 32, 32                \n",
    "                    thresh = imutils.resize(thresh, width=rsw) if tW > tH else \\\n",
    "                                    imutils.resize(thresh, height=rsh)\n",
    "                    (tH, tW) = thresh.shape\n",
    "                    dX = int(max(0, rsw - tW) / 2.0)\n",
    "                    dY = int(max(0, rsh - tH) / 2.0)\n",
    "\n",
    "                    # pad the image and force 32x32 dimensions\n",
    "                    padded = cv2.copyMakeBorder(thresh, top=dY, bottom=dY, left=dX, right=dX, \n",
    "                                                borderType=cv2.BORDER_CONSTANT,value=(0, 0, 0))\n",
    "                    padded = cv2.resize(padded, (rsw, rsh))\n",
    "                    padded = padded.astype(\"float32\")\n",
    "                    padded = np.expand_dims(padded, axis=-1)\n",
    "                    chars.append((padded, (x, y, w, h)))\n",
    "\n",
    "            chars = sorted(chars, key=lambda c: c[1][2], reverse=True)\n",
    "            return chars\n",
    "\n",
    "\n",
    "        def extract_contours_list_(img_list):\n",
    "            chars_list = None\n",
    "            for img in img_list:\n",
    "                cts = extract_contours_(img)\n",
    "                if len(cts) > 0:\n",
    "                    chars_list = [cts[0]] if chars_list is None else chars_list + [cts[0]]\n",
    "            # chars_list = sorted(chars_list, key=lambda c: c[1][2], reverse=True)\n",
    "\n",
    "            return chars_list\n",
    "\n",
    "\n",
    "        return extract_contours_list_(img_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def process_char(img, pos, num_chars=5, chop_wd = [0, -2, -4, 2, 4], chop_width=[0]):\n",
    "        img_rn = PredictCaptcha.remove_noise(img)\n",
    "        chop_imgs = PredictCaptcha.chop_multi_versions(img_rn, num_chars, chop_wd, chop_width)\n",
    "        all_chars = PredictCaptcha.extract_contours_all(chop_imgs[pos])\n",
    "\n",
    "        return all_chars\n",
    "\n",
    "\n",
    "    def predict(self, fimg):\n",
    "        format_y = lambda y: ''.join(map(lambda x: chr(int(x)), y))\n",
    "        res_chars = ''\n",
    "        y_pred_ls = []\n",
    "        y_pred_ls_all = []\n",
    "        for pos in range(5):\n",
    "            chars_img = self.process_char(fimg, pos, num_chars=5, chop_wd=[0, -2, 2], chop_width=[0, 2])\n",
    "            y_pred_ls = []\n",
    "            for cimg in chars_img:\n",
    "                im = np.array([np.repeat(x, 3, axis=1) for x in cimg[0]], np.uint8)\n",
    "                im = np.array(im) / 255.0\n",
    "                y_pred = self.fn_model.predict(np.array([im]), verbose=0)\n",
    "                am = tf.math.argmax(y_pred, axis=-1)\n",
    "                c = format_y(am[0])\n",
    "                y_pred_ls.append({'c': c, 'pc': y_pred[0][0][am][0][0]})\n",
    "\n",
    "            dft = pd.DataFrame.from_records(y_pred_ls)        \n",
    "            dft = dft.groupby('c')['pc'].agg(['count', 'max']).reset_index()\\\n",
    "                        .sort_values(by=['count'], ascending=False).head(2)\n",
    "\n",
    "            # fix the ad hoc case r, h\n",
    "            if set(dft['c']) == set(['r', 'h']):\n",
    "                top_char = 'r'\n",
    "            else: \n",
    "                top_char = dft.sort_values(by=['max'], ascending=False).head(1).iloc[0][0]\n",
    "\n",
    "            y_pred_ls_all.append(y_pred_ls)\n",
    "\n",
    "            res_chars += top_char\n",
    "\n",
    "        return {\n",
    "            'predict': res_chars, \n",
    "            'params': y_pred_ls_all\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da9b5671-e8d0-44f8-9703-b6538ad3ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import re\n",
    "import selenium\n",
    "\n",
    "class CrawlTax():\n",
    "            \n",
    "    def __init__(self, search_type=\"tax_num\", chrome_execute = None, chrome_download=None, \n",
    "                 driver=None, predict_captcha=None, download_dir_tax=DOWNLOAD_DIR_TAX_NUM):\n",
    "        self.chrome_execute = chrome_execute\n",
    "        self.chrome_download = chrome_download\n",
    "        self.driver = driver if driver is not None else self.restart_driver()        \n",
    "        self.predict_captcha = predict_captcha if predict_captcha is not None else PredictCaptcha()\n",
    "        self.search_type = search_type\n",
    "        self.list_captcha = []\n",
    "        self.download_dir_tax = download_dir_tax\n",
    "\n",
    "    def restart_driver(self):\n",
    "        capabilities = DesiredCapabilities.CHROME\n",
    "        capabilities[\"goog:loggingPrefs\"] = {\"performance\": \"ALL\"}\n",
    "\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--incognito\")\n",
    "        # chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "        chrome_options.add_argument('--ignore-certificate-errors')\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        download_folder = \"/Users/truong.vu.1/Documents/Smartpay/source/etl-airflow/data/download\"\n",
    "        prefs = {\"profile.default_content_settings.popups\": 0,    \n",
    "                \"download.default_directory\": self.chrome_download, \n",
    "                \"download.prompt_for_download\": False,\n",
    "                \"download.directory_upgrade\": True} if self.chrome_download is not None else None\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs) if prefs is not None else None\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        # chrome_options.add_argument('--start-maximized');\n",
    "        # chrome_options.add_argument('--start-fullscreen');\n",
    "        # chrome_options.add_argument('--window-size=1920,1200');\n",
    "        # chrome_options.add_argument(\"force-device-scale-factor=0.75\");\n",
    "        # chrome_options.add_argument(\"high-dpi-support=0.75\");\n",
    "        \n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager(\"114.0.5735.90\").install()),\\\n",
    "                                  options=chrome_options,\n",
    "                                    desired_capabilities=caps) if self.chrome_execute is None else \\\n",
    "                 webdriver.Chrome(service=Service(self.chrome_execute),\\\n",
    "                                  options=chrome_options,\n",
    "                                 desired_capabilities=capabilities)\n",
    "        return self.driver\n",
    "\n",
    "    def submit_tax_num(self, tax_num):\n",
    "        \n",
    "        self.driver.get(\"https://....\")\n",
    "        ip_tax = MyBrowseUtil.input_element(self.driver, tax_num, None, By.NAME, 'mst') \\\n",
    "                        if self.search_type == \"tax_num\" else \\\n",
    "                 MyBrowseUtil.input_element(self.driver, tax_num, None, By.NAME, 'cmt')\n",
    "        \n",
    "        \n",
    "        repeat_num = 5\n",
    "        while repeat_num > 0:\n",
    "            try:\n",
    "                body_captcha, captcha_text = None, None\n",
    "                repeat_num -= 1\n",
    "\n",
    "                # Get image captcha\n",
    "                process_browser_log_entry = lambda entry: json.loads(entry['message'])['message']\n",
    "                browser_log = self.driver.get_log('performance') \n",
    "                events = [process_browser_log_entry(entry) for entry in browser_log]\n",
    "                events = [event for event in events if 'Network.response' in event['method']]\n",
    "                check_event_captcha = lambda e: True if 'params' in e and 'response' in e['params'] \\\n",
    "                                                            and 'url' in e['params']['response'] \\\n",
    "                                                            and 'captcha' in e['params']['response']['url'] else False\n",
    "\n",
    "                reqid_captcha = [e for e in events if check_event_captcha(e)][0]['params']['requestId']\n",
    "                print('reqid_captcha', reqid_captcha)\n",
    "                body_captcha = self.driver.execute_cdp_cmd('Network.getResponseBody', {'requestId': reqid_captcha})\n",
    "                imgdata = base64.b64decode(str(body_captcha['body']))\n",
    "                img_captcha = Image.open(io.BytesIO(imgdata))\n",
    "\n",
    "                # Input tax and captcha\n",
    "                captcha_text = self.predict_captcha.predict(img_captcha)['predict']\n",
    "                print('captcha_text', captcha_text)\n",
    "                ip_captcha = MyBrowseUtil.input_element(self.driver, captcha_text, None, By.ID, 'captcha')\n",
    "                time.sleep(1)\n",
    "\n",
    "                # Click submit\n",
    "                click_btn = MyBrowseUtil.click_element_wt(self.driver, None, None, By.CLASS_NAME, 'subBtn')\n",
    "                table_tax = MyBrowseUtil.wait_element(self.driver, None, By.XPATH, '//table[@class=\"ta_border\"]')[0]\n",
    "                self.list_captcha.append({'img': str(body_captcha['body']), \n",
    "                                          'pred': captcha_text, \n",
    "                                          'result': 'PASSED'})\n",
    "                return table_tax\n",
    "            except selenium.common.exceptions.WebDriverException:\n",
    "                print('Exception Network.getResponseBody ')\n",
    "                self.driver.get(\"https://...\")                \n",
    "                ip_tax = MyBrowseUtil.input_element(self.driver, tax_num, None, By.NAME, 'mst') \\\n",
    "                        if self.search_type == \"tax_num\" else \\\n",
    "                         MyBrowseUtil.input_element(self.driver, tax_num, None, By.NAME, 'cmt')\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.list_captcha.append({'img': str(body_captcha['body']), \n",
    "                                          'pred': captcha_text, \n",
    "                                          'result': 'FAILED'}) \\\n",
    "                            if body_captcha is not None and captcha_text is not None else None\n",
    "                print('submit_tax_num', e)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def extract_table_(driver, x_path):\n",
    "        tbl_tax = MyBrowseUtil.wait_element(driver, None, By.XPATH, x_path)[0]    \n",
    "        rows = MyBrowseUtil.wait_element(tbl_tax, None, By.XPATH, \".//tr\")\n",
    "        step_try, df = 3, None\n",
    "        while step_try > 0:\n",
    "            step_try = step_try - 1\n",
    "\n",
    "            try:\n",
    "                cells = rows[0].find_elements(By.XPATH, \".//th | .//td\")\n",
    "                header_parse = [cell.text for cell in cells]\n",
    "\n",
    "                get_cell = lambda row: [cell for cell in row.find_elements(By.XPATH, \".//td | .//th\")]\n",
    "                rows_parse = [c for c in [get_cell(row) for row in rows] if len(c) > 0]\n",
    "                df = pd.DataFrame(rows_parse, columns=header_parse)\n",
    "            except Exception as e:\n",
    "                print('extract_table_', e, 'Table probably have not been loaded', step_try)\n",
    "                time.sleep(5)\n",
    "            \n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_df_rec_(df):\n",
    "        \n",
    "        dft = df.applymap(lambda x: x.text if x is not None else None)\n",
    "        part_1 = dict(zip(dft.iloc[:, 0], dft.iloc[:, 1]))\n",
    "        part_2 = dict(zip(dft.iloc[:, 2], dft.iloc[:, 3]))\n",
    "        part_3 = dict(zip(dft.iloc[:, 4], dft.iloc[:, 5]))\n",
    "\n",
    "        part_1.update(part_2)\n",
    "        part_1.update(part_3)\n",
    "        \n",
    "        return part_1\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_table_html_(html_table):\n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_table, 'html.parser')\n",
    "\n",
    "        # Find all <th> tags to extract field names\n",
    "        fields = [th.text.strip().replace('\\n', ' ') for th in soup.find_all('th')]\n",
    "\n",
    "        # Find all <tr> tags to extract data rows\n",
    "        data_rows = soup.find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "        # Create a list of dictionaries for each row\n",
    "        data = []\n",
    "        for row in data_rows:\n",
    "            row_data = [td.text.strip().replace('\\n',' ') for td in row.find_all('td')]\n",
    "            data.append(dict(zip(fields, row_data)))\n",
    "\n",
    "        # Convert the data to JSON\n",
    "        import json\n",
    "        json_data = json.dumps(data, ensure_ascii=False)\n",
    "\n",
    "        return json_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_link_(x, attr, search_text, tag='a'):\n",
    "        t = x.find_elements(By.XPATH, f'.//{tag}')\n",
    "        href = t[0].get_attribute(attr) if len(t) > 0 else None\n",
    "        return t[0] if href is not None and search_text in href else None\n",
    "    \n",
    "    @staticmethod\n",
    "    def run_js_(driver, tax_num):\n",
    "        async_script = f\"\"\"\n",
    "            javascript:submitform('{tax_num}')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            driver.execute_async_script(async_script)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def get_table_tax(self):\n",
    "        df = self.extract_table_(self.driver, '//table[@class=\"ta_border\"]')\n",
    "        df_list_tax = df.applymap(lambda x: x.text if x is not None else None)\n",
    "        df_list_tax.columns = [re.sub(r'[^a-zA-Z0-9_]', '_', unidecode(c).lower()).replace(\"__\", \"_\").replace(\"__\", \"_\") \\\n",
    "                if c is not None else \"empty_col\" for c in df_list_tax.columns]\n",
    "        df_list_tax = df_list_tax[[c for c in df_list_tax.columns if c!= 'empty_col']]\n",
    "        \n",
    "        # Get link details\n",
    "        # dft = df.applymap(lambda x: self.find_link_(x, 'href', 'submit_form') if x is not None else None).stack()\n",
    "        # link_details = dft[dft.notna()].tolist()\n",
    "        \n",
    "        # Remove first and last row\n",
    "        df_list_tax = df_list_tax.iloc[1:-1,]\n",
    "        \n",
    "        return df_list_tax\n",
    "        \n",
    "    def full_page_screenshot(self, tax_num):\n",
    "        \n",
    "        height = self.driver.execute_script('return document.documentElement.scrollHeight')\n",
    "        width  = self.driver.execute_script('return document.documentElement.scrollWidth')\n",
    "        self.driver.set_window_size(width, height)\n",
    "        # self.driver.execute_script(\"window.scrollTo(0, 300)\")\n",
    "        time.sleep(1)\n",
    "        self.driver.save_screenshot(self.download_dir_tax + 'tax_ss_' + str(tax_num).strip() + '.jpg')\n",
    "        \n",
    "        \n",
    "    def get_table_tax_detail(self, tax_num):\n",
    "        \n",
    "        # link_detail.click()\n",
    "        # \n",
    "        \n",
    "        # Get screenshot\n",
    "        self.full_page_screenshot(tax_num)\n",
    "        df = self.extract_table_(self.driver, '//table[@class=\"ta_border\"]')        \n",
    "        records_df = self.convert_df_rec_(df)\n",
    "        df_final = pd.DataFrame.from_records([records_df])\n",
    "        df_final.columns = [re.sub(r'[^a-zA-Z0-9_]', '_', unidecode(c).lower()).replace(\"__\", \"_\").replace(\"__\", \"_\") \\\n",
    "                if c is not None else \"empty_col\" for c in df_final.columns]\n",
    "        df_final = df_final[[c for c in df_final.columns if c!= 'empty_col']]\n",
    "        return df_final\n",
    "\n",
    "\n",
    "    # def get_table_tax_extra(self, tax_num):\n",
    "    #     df = self.extract_table_(self.driver, '//form[@id=\"loadFrm\"]//table')\n",
    "    #     dft = df.applymap(lambda x: self.find_link_(x, 'link', '.jsp', 'input') if x is not None else None).stack()\n",
    "    #     link_extras = dft[dft.notna()].tolist()\n",
    "\n",
    "    #     ls_extra = {}\n",
    "    #     for ld in link_extras:\n",
    "    #         url = ld.get_attribute('link') + f'?tin={tax_num}'\n",
    "    #         html_table = MyBrowseUtil.call_api(self.driver, url, {})\n",
    "    #         key_url = ld.get_attribute('link').split(\".jsp\")[0].split(\"/\")[-1]\n",
    "    #         json_table = self.parse_table_html_(html_table)\n",
    "    #         print('url link extra', url)\n",
    "    #         ls_extra.update({key_url: json_table})\n",
    "\n",
    "    #     return ls_extra\n",
    "    \n",
    "    def get_table_tax_extra(self, tax_num): \n",
    "        \n",
    "        # /tcnnt/doanhnghiepchuquan.jsp?tin=2800214981\n",
    "        # /tcnnt/chinhanh.jsp?tin=2800214981\n",
    "        # /tcnnt/tructhuoc.jsp?tin=2800214981\n",
    "        # /tcnnt/daidien.jsp?tin=2800214981\n",
    "        # /tcnnt/loaithue.jsp?tin=2800214981\n",
    "        # /tcnnt/nganhkinhdoanh.jsp?tin=2800214981\n",
    "        \n",
    "        link_extras = ['/tcnnt/nganhkinhdoanh.jsp', \n",
    "                       '/tcnnt/loaithue.jsp']        \n",
    "        ls_extra = {}\n",
    "        \n",
    "        for ld in link_extras:            \n",
    "            url = ld + f'?tin={tax_num}'\n",
    "            html_table = MyBrowseUtil.call_api(self.driver, url, {})\n",
    "            key_url = ld.split(\".jsp\")[0].split(\"/\")[-1]\n",
    "            json_table = self.parse_table_html_(html_table)\n",
    "            print('url link extra', url)\n",
    "            ls_extra.update({key_url: json_table})\n",
    "        \n",
    "        return ls_extra\n",
    "    \n",
    "    def get_detail_extra(self, tax_num):\n",
    "        try:                        \n",
    "            # Run js\n",
    "            print(tax_num)\n",
    "            self.run_js_(self.driver, tax_num)\n",
    "            time.sleep(2)\n",
    "            df_detail = self.get_table_tax_detail(tax_num)        \n",
    "            ls_extra = self.get_table_tax_extra(tax_num)\n",
    "\n",
    "            # Merge detail & extra\n",
    "            df_extra = pd.DataFrame.from_records([ls_extra])\n",
    "            df_detail_extra = pd.concat([df_detail.reset_index(drop=True), df_extra.reset_index(drop=True)], \n",
    "                                        axis=1)\n",
    "            return df_detail_extra\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "        \n",
    "    \n",
    "    def run_one_tax(self, tax_num):\n",
    "        repeat_num = 5\n",
    "        while repeat_num > 0:\n",
    "            repeat_num -= 1\n",
    "            try:\n",
    "                # Get tax\n",
    "                self.submit_tax_num(tax_num)\n",
    "                df_tax = self.get_table_tax()\n",
    "\n",
    "                if len(df_tax) == 0:\n",
    "                    df_final = pd.DataFrame({'id': [str(tax_num).strip() + '_' + self.search_type[0:3]],\n",
    "                                             'org_search_key': [str(tax_num).strip()],\n",
    "                                             'org_search_type': [self.search_type],\n",
    "                                             'status': ['Non_Exist']})\n",
    "                else:                    \n",
    "                    \n",
    "                    # Get tax detail\n",
    "                    print('Get tax detail and extra ', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "                    ls_df_detail_extra = df_tax.iloc[:3,].apply(lambda x:  self.get_detail_extra(x['mst']), \n",
    "                                                                axis=1)\n",
    "                    df_detail_extra = pd.concat(ls_df_detail_extra.to_list())\n",
    "\n",
    "                    # Merge\n",
    "                    print('Merge tax list & tax detail & extra', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "                    df_final = pd.merge(df_tax, df_detail_extra, how = \"left\", \n",
    "                                        left_on=\"mst\", right_on=\"ma_so_doanh_nghiep\")\n",
    "                    df_final['id'] = str(tax_num).strip() + '_' + self.search_type[0:3] + \\\n",
    "                                                            '_' + df_final['mst']\n",
    "                    df_final['org_search_key'] = str(tax_num).strip()\n",
    "                    df_final['org_search_type'] = self.search_type\n",
    "                    df_final['status'] = 'Exist'\n",
    "\n",
    "                return df_final\n",
    "            except Exception as e:\n",
    "                print('run_one_tax - repeat :', repeat_num, e)\n",
    "                time.sleep(5)\n",
    "                self.driver.close()\n",
    "                self.restart_driver()\n",
    "                \n",
    "            return None\n",
    "                \n",
    "    def run_list_tax(self, list_tax_num):\n",
    "        ls_df = []\n",
    "        for i, tn in enumerate(list_tax_num):\n",
    "            print('Processing -- ', i, tn, datetime.now().strftime(\"%H:%M:%S\"))\n",
    "            df_tax_num = self.run_one_tax(str(tn).strip())\n",
    "            ls_df.append(df_tax_num) if df_tax_num is not None else None\n",
    "        \n",
    "        return pd.concat(ls_df) if len(ls_df) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6f867f5-1bd4-45b7-aeb4-ebd432b64c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if df_input is None:\n",
    "#     class StopExecution(Exception):\n",
    "#         def _render_traceback_(self):\n",
    "#             pass\n",
    "\n",
    "#     print('No exists files')\n",
    "#     raise StopExecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee836ab-153b-4b38-8a3f-f20b2db54069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import urllib.request\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def run_list(col_search, tax_list):\n",
    "    crawl_tax = CrawlTax(col_search, CHROME_EXECUTE_EMS)\n",
    "    df_list = crawl_tax.run_list_tax(tax_list)\n",
    "    return (df_list, crawl_tax.list_captcha)\n",
    "\n",
    "\n",
    "while True:\n",
    "    dir_upload = SFTP_TAX_TO_CRAWL\n",
    "    df_input, fn_processing = read_sftp(SFTP_BIS3, dir_upload, dir_upload + '/completed/', str, \n",
    "                                        [\"csv\", \"xlsx\"], None, True, None, {'RETURN_FILE_NAME': True})\n",
    "    \n",
    "    # df_input = pd.DataFrame({'tax_num': ['0315097884']})\n",
    "    \n",
    "    if df_input is None:\n",
    "        print('No exists files')\n",
    "        break\n",
    "    \n",
    "    print('---------------- Processing file ------', fn_processing)\n",
    "\n",
    "    col_search = [c for c in df_input.columns if 'tax_num' in c or 'national_id' in c][0]\n",
    "    tax_list_all = list(df_input[col_search].unique())\n",
    "    col_search = 'tax_num' if 'tax_num' in col_search else 'national_id'\n",
    "\n",
    "    print('col_search, tax_list_all', col_search, tax_list_all[0:5])\n",
    "\n",
    "    batch_save = 20\n",
    "    for i in range(0, len(tax_list_all), batch_save):\n",
    "\n",
    "        # Batch to save\n",
    "        tax_list = tax_list_all[i:i+batch_save]\n",
    "\n",
    "        # Running one batch\n",
    "        ls_batch_res, lsfn_captcha, batch_size = [], [], 20\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "\n",
    "            exe_batch = {executor.submit(run_list, col_search, tax_list[b:b+batch_size]): b \\\n",
    "                                     for b in range(0, len(tax_list), batch_size)}\n",
    "            for future in concurrent.futures.as_completed(exe_batch):\n",
    "                batch = exe_batch[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                    ls_batch_res.append(data[0]) if data[0] is not None else None\n",
    "                    lsfn_captcha += data[1]\n",
    "                except Exception as e:\n",
    "                    raise\n",
    "                    print('ThreadPoolExecutor Exception: ', batch, e)\n",
    "                else:\n",
    "                    print('------------- Batch - Len data - Total batch', batch, len(data), len(ls_batch_res))\n",
    "\n",
    "        # Write list\n",
    "        print('Save batch list')\n",
    "        df_list = pd.concat(ls_batch_res).reset_index(drop=True) if len(ls_batch_res)>0 else None\n",
    "        dt = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "        file_upload = SFTP_TAX_CRAWL_COMPLETE + f'tax_crawl_{dt}.csv'\n",
    "        write_sftp(SFTP_BIS3,\n",
    "                   df_list, \n",
    "                   file_upload,\n",
    "                   option={'csv': True}) if df_list is not None else None\n",
    "        df_captcha = pd.DataFrame.from_records(lsfn_captcha)\n",
    "\n",
    "        # Write captcha\n",
    "        clear_output(wait=True)\n",
    "        print('Save batch captcha')\n",
    "        dt = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "        file_upload = SFTP_TAX_CAPTCHA + f'tax_captcha_{dt}.csv'\n",
    "        write_sftp(SFTP_BIS3, \n",
    "                   df_captcha, \n",
    "                   file_upload,\n",
    "                   option={'csv': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ade94-a5cf-4d3a-93da-f35f4d42bed5",
   "metadata": {},
   "source": [
    "#### Transfer files tax to sftp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84859a8e-ea77-4f35-8a3d-174d30d050fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy tax screenshot to sftp\n",
    "transfer_sftp(SFTP_BIS3, SFTP_UPLOAD_TAX_NUM, DOWNLOAD_DIR_TAX_NUM, other_params={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a55018-5164-4659-8766-cb7804eb88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move all files to complete\n",
    "move_dir(DOWNLOAD_DIR_TAX_NUM, DOWNLOAD_DIR_TAX_NUM_COMPLETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2a949-d6bf-4be4-baba-864ed1434981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl_tax = CrawlTax(CHROME_EXECUTE_EMS)\n",
    "\n",
    "# df_list = crawl_tax.run_list_tax(tax_list)\n",
    "# df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b68618-9461-41d1-a690-68985c6adab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list = pd.concat(ls_batch_res).reset_index(drop=True)\n",
    "# df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd62f0e-8c0a-4ea6-b2aa-979ce782d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "# file_upload = f'/processing/bi_upload/ftp_to_db/tax_crawl/tax_crawl_{dt}.csv'\n",
    "# write_sftp(SFTP_BIS3, \n",
    "#            df_list, \n",
    "#            file_upload,\n",
    "#            option={'csv': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea5abf-14e3-4010-ac1d-a022f3e601f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_captcha = pd.DataFrame.from_records(lsfn_captcha)\n",
    "# df_captcha.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ba8876-f9c2-4f27-8620-738a74c95cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "# file_upload = f'/processing/bi_upload/ftp_to_db/tax_captcha/tax_captcha_{dt}.csv'\n",
    "# write_sftp(SFTP_BIS3, \n",
    "#            df_captcha, \n",
    "#            file_upload,\n",
    "#            option={'csv': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20983c-c13d-40b8-96ef-f287bdc2356a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfabdc5-9e7c-4ef1-a008-e47a9e51eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BACKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c40aad-6202-4f1f-9fbd-f2fa1b3dfb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# str_tax = \\\n",
    "# \"\"\"\n",
    "# 092089003051\n",
    "# 381593913\n",
    "# 024889775\n",
    "# 023350487\n",
    "# 381700684\n",
    "# 142578753\n",
    "# 351315401\n",
    "# 077089000860\n",
    "# 341647711\n",
    "# \"\"\"\n",
    "\n",
    "# list_tax = list(set(str_tax.split(\"\\n\")))\n",
    "# list_tax\n",
    "\n",
    "\n",
    "# dir_upload = f'/processing/bi_upload/db_to_ftp/tax_crawl/'\n",
    "# df_list = pd.DataFrame({'national_id': list_tax})\n",
    "# file_upload = f'{dir_upload}tax_crawl_2.csv'\n",
    "# write_sftp(SFTP_BIS3,\n",
    "#            df_list, \n",
    "#            file_upload,\n",
    "#            option={'csv': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9c951-4257-4f7e-8fae-99296db892a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

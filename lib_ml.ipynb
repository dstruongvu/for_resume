{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ca0f0-f3ba-45a5-be69-8036df78837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "from hyperopt import space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c73fb46-af6c-4e9a-aa13-56846c43288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, method=\"median\"):\n",
    "        self._method = method\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        num_cols = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        self._train_cols = df_train.columns.to_list()\n",
    "        \n",
    "        self._impute_values = {}\n",
    "        for col in num_cols:\n",
    "            self._impute_values[col] = df_train[col].agg(self._method)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        cols = df.columns.to_list()\n",
    "        assert set(cols) == set(self._train_cols), \"Do not have the same set of cols as train\"\n",
    "        \n",
    "        for col, val in self._impute_values.items():\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                df[col] = df[col].fillna(val)\n",
    "        \n",
    "        # align columns\n",
    "        df = df[self._train_cols]\n",
    "        return df\n",
    "    \n",
    "\n",
    "class CatImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, val=\"MISSING\"):\n",
    "        self._val = val\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        cat_cols = df_train.select_dtypes([\"object\", \"category\", \"bool\"]).columns.to_list()\n",
    "        self._train_cols = df_train.columns.to_list()\n",
    "        \n",
    "        self._impute_values = {}\n",
    "        for col in cat_cols:\n",
    "            self._impute_values[col] = self._val\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        cols = df.columns.to_list()\n",
    "        assert set(cols) == set(self._train_cols), \"Do not have the same set of cols as train\"\n",
    "        \n",
    "        for col, val in self._impute_values.items():\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                df[col] = df[col].astype(\"object\").fillna(val).astype(\"category\")\n",
    "                \n",
    "        # align columns\n",
    "        df = df[self._train_cols]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fbef7f-b02d-4614-b8ab-9a353adc4bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(estimator, features):\n",
    "    \"\"\"\n",
    "    :param estimator: an estimator object that has feature_importances_ attribute\n",
    "    :param features: list of str, list of feature names\n",
    "    :return: feature_imp, dataframe\n",
    "    \"\"\"\n",
    "    feature_imp = pd.DataFrame({\"feature\": features, \"importance\": estimator.feature_importances_})\n",
    "    feature_imp = feature_imp.sort_values(by=[\"importance\"], ascending=False)\n",
    "    \n",
    "    feature_imp[\"rank\"] = np.arange(feature_imp.shape[0]) + 1\n",
    "    return feature_imp\n",
    "\n",
    "\n",
    "def roc_auc(estimator, X_eval, y_eval):\n",
    "    \"\"\"\n",
    "    :param estimator: sklearn estimator that have predict_proba() method\n",
    "    :param X_eval: test features\n",
    "    :param y_eval: test target\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X_eval)\n",
    "    return roc_auc_score(y_eval, proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba047533-b3f2-440c-8cd1-185c390433f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class CF():    \n",
    "        \n",
    "    def __init__(self, Y_data, k, filename_sims = \"hdfs://master:9000/test/similarity.parquet\"):\n",
    "        self.k_nearest = k\n",
    "        self.Ybar_data = Y_data\n",
    "        self.S = None\n",
    "        self.mu = None\n",
    "        self.filename_sims = filename_sims\n",
    "        \n",
    "    \n",
    "    def normalize_Y(self):\n",
    "        from pyspark.sql.functions import monotonically_increasing_id \n",
    "        import pyspark.sql.functions as F\n",
    "        import numpy as np\n",
    "        from numpy.linalg import norm\n",
    "        from pyspark.sql.types import IntegerType, DoubleType, StructField, StringType, FloatType, StructType\n",
    "        from pyspark.sql.window import Window\n",
    "        \n",
    "        print('Doing normalize_Y', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        \n",
    "        windowSpec  = Window.partitionBy(\"user\")\n",
    "        self.Ybar_data = self.Ybar_data.withColumn(\"rating\", F.round((F.col(\"rating\") - F.mean(\"rating\").over(windowSpec)) \n",
    "                                              / (F.stddev_pop(\"rating\").over(windowSpec) + 1e-8) + 3, 2))\n",
    "    \n",
    "    def pivot_save(self, filename_pivot):\n",
    "        import pyspark.sql.functions as F\n",
    "        import numpy as np\n",
    "        from numpy.linalg import norm\n",
    "        from pyspark.sql.types import IntegerType, DoubleType, StructField, StringType, FloatType, StructType\n",
    "        from pyspark.sql.window import Window\n",
    "        \n",
    "        self.normalize_Y()\n",
    "        w = Window.orderBy('mid')\n",
    "        ps_pivot = self.Ybar_data.groupBy(\"user\").pivot(\"item\").sum(\"rating\").fillna(0).select(\"*\")\n",
    "        ps_pivot = ps_pivot.withColumn(\"mid\", F.monotonically_increasing_id())\\\n",
    "                            .withColumn(\"row_idx\", F.row_number().over(w) - 1)\\\n",
    "                            .drop('mid')\n",
    "        ps_pivot.write.mode(\"overwrite\").parquet(filename_pivot)\n",
    "        \n",
    "    def similarity(self, filename_pivot=None):\n",
    "        import pyspark.sql.functions as F\n",
    "        import numpy as np\n",
    "        from numpy.linalg import norm\n",
    "        from pyspark.sql.types import IntegerType, DoubleType, StructField, StringType, FloatType, StructType\n",
    "        from pyspark.sql.window import Window\n",
    "        from pyspark.ml.linalg import Vectors\n",
    "        from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "        import gc\n",
    "        \n",
    "        print('Doing similarity', datetime.now().strftime(\"%H:%M:%S\"))   \n",
    "        \n",
    "        # pivot table\n",
    "        if filename_pivot is None:\n",
    "            w = Window.orderBy('mid')\n",
    "            ps_pivot = self.Ybar_data.groupBy(\"user\").pivot(\"item\").sum(\"rating\").fillna(0).select(\"*\")\n",
    "            ps_pivot = ps_pivot.withColumn(\"mid\", F.monotonically_increasing_id())\\\n",
    "                                .withColumn(\"row_idx\", F.row_number().over(w) - 1)\\\n",
    "                                .drop('mid')\\\n",
    "                                .checkpoint()\n",
    "        else:\n",
    "            ps_pivot = spark_ss.read.parquet(filename_pivot)\n",
    "\n",
    "# First \n",
    "#         cols = list(set(ps_pivot.columns) - set(['row_idx', 'user']))\n",
    "#         dot_udf = F.udf(lambda x: cosine(x[0], x[1]), DoubleType())\n",
    "#         def cosine(A, B):\n",
    "#             return round(float(np.dot(np.array(A), np.array(B))/ (norm(np.array(A))*norm(np.array(B)))), 2)\n",
    "\n",
    "#         sims = ps_pivot.alias(\"a\").join(ps_pivot.alias(\"b\"), F.col(\"a.row_idx\") < F.col(\"b.row_idx\"))\\\n",
    "#                 .select([F.col(f\"a.{c}\").alias(f\"a_{c}\") for c in ps_pivot.columns] + \\\n",
    "#                         [F.col(f\"b.{c}\").alias(f\"b_{c}\") for c in ps_pivot.columns]) \\\n",
    "#                 .withColumn(\"sim\", dot_udf(F.struct([\n",
    "#                     F.struct([F.col(f\"a_{col}\") for col in cols]), \\\n",
    "#                     F.struct([F.col(f\"b_{col}\") for col in cols])])))\\\n",
    "#                 .select(F.col(\"a_row_idx\").alias(\"i\"), F.col(\"b_row_idx\").alias(\"j\"), F.col(\"sim\"))\n",
    "\n",
    "# Second \n",
    "#         ps_pivot = ps_pivot.withColumn(\"joint_cols\", F.array(*cols))\n",
    "#         dot_udf = F.udf(lambda A, B: round(float(np.dot(np.array(A),\n",
    "#                                                     np.array(B))/\n",
    "#                                              (norm(np.array(A))*norm(np.array(B)))), 2), \n",
    "#                           DoubleType())\n",
    "#         sims = ps_pivot.alias(\"i\").join(ps_pivot.alias(\"j\"), F.col(\"i.row_idx\") < F.col(\"j.row_idx\"))\\\n",
    "#                 .select(\n",
    "#                     F.col(\"i.row_idx\").alias(\"i\"), \n",
    "#                     F.col(\"j.row_idx\").alias(\"j\"), \n",
    "#                     dot_udf(\"i.joint_cols\", \"j.joint_cols\").alias(\"sim\"))\n",
    "\n",
    "# Third \n",
    "#         cols = list(set(ps_pivot.columns) - set(['row_idx', 'user', 'joint_cols']))        \n",
    "#         ps_pivot = ps_pivot.withColumn(\"joint_cols\", F.array(*cols))\n",
    "#         sims = ps_pivot.alias(\"i\").join(ps_pivot.alias(\"j\"), F.col(\"i.row_idx\") < F.col(\"j.row_idx\"))\\\n",
    "#                 .withColumn(\"dot_prod\", F.lit(sum([F.col(\"i.joint_cols\")[i] * F.col(\"j.joint_cols\")[i] for i in range(len(cols))]))) \\\n",
    "#                 .withColumn(\"norm_1\", F.lit(F.sqrt(sum([F.col(\"i.joint_cols\")[i] * F.col(\"i.joint_cols\")[i] for i in range(len(cols))])))) \\\n",
    "#                 .withColumn(\"norm_2\", F.lit(F.sqrt(sum([F.col(\"j.joint_cols\")[i] * F.col(\"j.joint_cols\")[i] for i in range(len(cols))])))) \\\n",
    "#                 .withColumn(\"sim\", F.lit(F.round(F.col(\"dot_prod\") / (F.col(\"norm_1\") * F.col(\"norm_2\")), 2))) \\\n",
    "#                 .select(F.col(\"i.user\").alias(\"id_1\"), F.col(\"j.user\").alias(\"id_2\"), F.col(\"sim\"))\\\n",
    "#                 .filter(~F.col(\"sim\").isNull() & ~F.isnan(F.col(\"sim\")) & (F.col(\"sim\") != 0))\\\n",
    "#                 .write.mode(\"overwrite\").parquet(self.filename_sims)\n",
    "\n",
    "# Fourth \n",
    "#         mat = IndexedRowMatrix(\n",
    "#                 ps_pivot.select(['row_idx'] + [c for c in ps_pivot.columns if c not in ['user', 'row_idx']])\\\n",
    "#                     .rdd.map(lambda row: IndexedRow(row[0], row[1:])))\n",
    "#         sims = spark_ss.createDataFrame(mat.columnSimilarities().entries)    \n",
    "#         sims.join(ps_pivot, sims.i == ps_pivot.row_idx, \"left\").select(F.col(\"i\"), \n",
    "#                                                                      F.col(\"j\"), \n",
    "#                                                                      F.col(\"user\").alias(\"id_1\"),\n",
    "#                                                                     F.col(\"value\"))\\\n",
    "#             .join(ps_pivot, sims.j == ps_pivot.row_idx, \"left\").select(F.col(\"id_1\"),\n",
    "#                                                                              F.col(\"user\").alias(\"id_2\"),\n",
    "#                                                                             F.col(\"value\").alias(\"sim\"))\\\n",
    "#             .filter(~F.col(\"sim\").isNull() & ~isnan(F.col(\"sim\")) & (F.col(\"sim\") != 0))\\\n",
    "#             .write.mode(\"overwrite\").parquet(self.filename_sims)\n",
    "\n",
    "# Fifth\n",
    "#         total = ps_pivot.count()\n",
    "#         pst, step = None, 7000\n",
    "#         for i in range(0, total, step):            \n",
    "#             limit_ps = min(total, i+step)\n",
    "#             tail_ps = min(total-i, step)\n",
    "#             print(i, \"/\", total, \"---\", limit_ps, tail_ps)\n",
    "\n",
    "#             # Get chunk of data to do similarities            \n",
    "#             pst = spark_ss.createDataFrame(ps_pivot.limit(limit_ps).tail(tail_ps))\n",
    "#             mat = IndexedRowMatrix(pst.select(['row_idx'] + [c for c in pst.columns if c not in ['user', 'row_idx']])\\\n",
    "#                 .rdd.map(lambda row: IndexedRow(row[0], row[1:])))\n",
    "#             sims = spark_ss.createDataFrame(mat.columnSimilarities().entries)\n",
    "#             spf = sims.join(pst, sims.i == pst.row_idx, \"left\").select(F.col(\"i\"), \n",
    "#                                                                                  F.col(\"j\"), \n",
    "#                                                                                  F.col(\"user\").alias(\"id_1\"),\n",
    "#                                                                                 F.col(\"value\"))\\\n",
    "#                         .join(pst, sims.j == pst.row_idx, \"left\").select(F.col(\"id_1\"),\n",
    "#                                                                                          F.col(\"user\").alias(\"id_2\"),\n",
    "#                                                                                         F.col(\"value\").alias(\"sim\"))\\\n",
    "#                         .filter(~F.col(\"sim\").isNull() & ~F.isnan(F.col(\"sim\")) & (F.col(\"sim\") != 0))\n",
    "#             spf.write.mode(\"overwrite\").parquet(self.filename_sims) if i == 0 else \\\n",
    "#                 spf.write.mode(\"append\").parquet(self.filename_sims)\n",
    "            \n",
    "#             del mat, sims, spf, pst\n",
    "#             gc.collect()\n",
    "\n",
    "        total = ps_pivot.count()\n",
    "        step = 10000\n",
    "        cols = ['idx'] + [c for c in ps_pivot.columns if c not in ['user', 'row_idx', 'idx']]\n",
    "\n",
    "        for i in range(0, total, step):\n",
    "            print(i, \"/\", total)\n",
    "\n",
    "            # Get chunk of data to do similarities\n",
    "            w = Window.orderBy('row_idx')\n",
    "            pst = ps_pivot.filter((F.col('row_idx') >= i) & (F.col('row_idx') < i+step))\\\n",
    "                            .withColumn(\"idx\", F.row_number().over(w) - 1).select('*')\n",
    "\n",
    "            mat = IndexedRowMatrix(pst.rdd.map(lambda row: IndexedRow(row[0], row[1:])))\n",
    "            sims = spark_ss.createDataFrame(mat.columnSimilarities().entries)\n",
    "            spf = sims.join(pst, sims.i == pst.idx, \"left\").select(F.col(\"i\"), \n",
    "                                                                                 F.col(\"j\"), \n",
    "                                                                                 F.col(\"user\").alias(\"id_1\"),\n",
    "                                                                                F.col(\"value\"))\\\n",
    "                        .join(pst, sims.j == pst.idx, \"left\").select(F.col(\"id_1\"),\n",
    "                                                                                         F.col(\"user\").alias(\"id_2\"),\n",
    "                                                                                        F.col(\"value\").alias(\"sim\"))\\\n",
    "                        .filter(~F.col(\"sim\").isNull() & ~F.isnan(F.col(\"sim\")) & (F.col(\"sim\") != 0))\n",
    "\n",
    "            spf.write.mode(\"overwrite\").parquet(self.filename_sims) if i == 0 else \\\n",
    "                spf.write.mode(\"append\").parquet(self.filename_sims)\n",
    "\n",
    "            del mat, sims, spf, pst\n",
    "            gc.collect()\n",
    "    \n",
    "        self.S = spark_ss.read.parquet(self.filename_sims)\n",
    "    \n",
    "\n",
    "    def refresh(self, normalized = True):\n",
    "        \"\"\"\n",
    "        Normalize data and calculate similarity matrix again (after\n",
    "        some few ratings added)\n",
    "        \"\"\"\n",
    "        self.normalize_Y() if normalized else None\n",
    "        self.similarity()\n",
    "        \n",
    "    def fit(self):\n",
    "        self.refresh()\n",
    "        \n",
    "    def load_sim(self, filename_sims = None):\n",
    "        if filename_sims is not None:\n",
    "            self.filename_sims = filename_sims\n",
    "        self.S = spark_ss.read.parquet(self.filename_sims)\n",
    "        \n",
    "    def __pred(self, u, i, ps_ratings):\n",
    "        import pyspark.sql.functions as F\n",
    "\n",
    "        # Filter sims match\n",
    "        sims_match = self.S.filter((self.S.id_1 == u) | (self.S.id_2 == u)).toPandas()\n",
    "        if len(sims_match) == 0:\n",
    "            return None\n",
    "        sims_match['id_x'] = sims_match.apply(lambda x: x['id_2'] if x['id_1'] == u else x['id_1'], axis=1)\n",
    "        ls_ids = list(sims_match['id_x'].values)\n",
    "\n",
    "        # Filter ratings\n",
    "        ratings = ps_ratings.filter(F.col(\"user\").isin(ls_ids) & (F.col(\"item\") == i)).toPandas()\n",
    "\n",
    "        # Merge sims and ratings\n",
    "        sim_ratings = ratings.merge(sims_match, left_on='user', right_on='id_x')\n",
    "        sim_nearest = sim_ratings.sort_values(by=['sim'], ascending=False).head(self.k_nearest)\n",
    "\n",
    "        if len(sim_nearest) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            r = np.array(list(sim_nearest['rating']))\n",
    "            s = np.array(list(sim_nearest['sim']))\n",
    "            ret = (r*s).sum()/(np.abs(s).sum() + 1e-8)\n",
    "            \n",
    "            return round(ret, 2)\n",
    "    \n",
    "    \n",
    "    def pred(self, u, i):\n",
    "        \"\"\" \n",
    "        predict the rating of user u for item i (normalized)\n",
    "        if you need the un\n",
    "        \"\"\"\n",
    "        return self.__pred(u, i, self.Ybar_data)\n",
    "    \n",
    "    \n",
    "    def recommend(self, u):\n",
    "        return self.__recommend(u, self.Ybar_data)\n",
    "\n",
    "    \n",
    "    def __recommend(self, u, ps_ratings):\n",
    "\n",
    "        ratings_ret = []\n",
    "\n",
    "        # Get list items\n",
    "        items = list(set(ps_ratings.select('item').rdd.flatMap(lambda x: x).collect()))\n",
    "\n",
    "        # Filter sims match\n",
    "        sims = self.S\n",
    "        sims_match = sims.filter((sims.id_1 == u) | (sims.id_2 == u)).toPandas()\n",
    "        if len(sims_match) == 0:\n",
    "            return []\n",
    "        sims_match['id_x'] = sims_match.apply(lambda x: x['id_2'] if x['id_1'] == u else x['id_1'], axis=1)\n",
    "        ls_ids = list(sims_match['id_x'].values)\n",
    "        ls_ids = list(sims_match['id_x'].values) + [u]\n",
    "\n",
    "        # Filter ratings\n",
    "        ratings = ps_ratings.filter(F.col(\"user\").isin(ls_ids)).toPandas()\n",
    "        rated_items = ratings[ratings['user'] == u]['item'].unique()\n",
    "        ratings = ratings[ratings['user'] != u]\n",
    "\n",
    "        # Merge sims and ratings\n",
    "        sim_ratings = ratings.merge(sims_match, left_on='user', right_on='id_x')\n",
    "\n",
    "        # Rating items not yet rating\n",
    "        for i in items:\n",
    "            if i not in rated_items:\n",
    "                sim_nearest = sim_ratings[sim_ratings['item'] == i].sort_values(by=['sim'], \n",
    "                                                                                ascending=False).head(self.k_nearest)\n",
    "                if len(sim_nearest) > 0:\n",
    "                    r = np.array(list(sim_nearest['rating']))\n",
    "                    s = np.array(list(sim_nearest['sim']))\n",
    "                    ret = (r*s).sum()/(np.abs(s).sum() + 1e-8)\n",
    "\n",
    "                    ratings_ret.append((u, i, round(float(ret), 2)))\n",
    "        return ratings_ret\n",
    "    \n",
    "    \n",
    "    def recommend_all(self, top_product=None, db_write=None, file_recommend=None):\n",
    "        import pyspark.sql.functions as F\n",
    "        import numpy as np\n",
    "        from numpy.linalg import norm\n",
    "        from pyspark.sql.types import IntegerType, DoubleType, StructField, StringType, FloatType, StructType\n",
    "        from pyspark.sql.window import Window\n",
    "        from pyspark.ml.linalg import Vectors\n",
    "        from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "        import gc\n",
    "        \n",
    "        return self.__recommend_all(top_product, db_write, file_recommend)\n",
    "    \n",
    "    \n",
    "    def __recommend_all_pandas(self, ps_ratings, top_product=None):\n",
    "        df_new_rating = None\n",
    "\n",
    "        # Rating dataframe\n",
    "        dft = ps_ratings.select('user', 'item', 'rating').to_pandas_on_spark()\n",
    "\n",
    "        # \n",
    "        for i in range(len(dft)):\n",
    "            x = dft.loc[i, :]\n",
    "            ratings = self.__recommend(x['user'], ps_ratings)\n",
    "            if len(ratings) > 0:\n",
    "                schema = StructType([\n",
    "                    StructField(\"user_id\", StringType(), True),\n",
    "                    StructField(\"item_id\", StringType(), True),\n",
    "                    StructField(\"predict_rating\", DoubleType(), True)])\n",
    "                newRow = spark_ss.createDataFrame(data=ratings, schema=schema)\n",
    "                df_new_rating = newRow if df_new_rating is None else df_new_rating.union(newRow)\n",
    "                \n",
    "        if top_product is not None:\n",
    "            windowDept = Window.partitionBy(\"user_id\").orderBy(F.col(\"item_id\").desc())\n",
    "            df_new_rating = df_new_rating.withColumn(\"top\", \\\n",
    "                                F.row_number().over(windowDept)).filter(F.col(\"top\") <= top_product) \\\n",
    "                                .select(\"user_id\", \"item_id\", \"rating\")\n",
    "\n",
    "        return df_new_rating\n",
    "    \n",
    "    \n",
    "    def recommend_all_users(self, top_product=None, ps_pivot=None):\n",
    "        \n",
    "        import pyspark.sql.functions as F\n",
    "        import numpy as np\n",
    "        from numpy.linalg import norm\n",
    "        from pyspark.sql.types import IntegerType, DoubleType, StructField, StringType, FloatType, StructType\n",
    "        from pyspark.sql.window import Window\n",
    "        from pyspark.ml.linalg import Vectors\n",
    "        from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "        import gc\n",
    "        \n",
    "        ps_ratings = self.Ybar_data\n",
    "        sims = self.S\n",
    "        ps_ret = None\n",
    "        \n",
    "        # pivot ratings\n",
    "        if ps_pivot is None:\n",
    "            print('Building pivot ', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "            wd = Window.orderBy('midx')\n",
    "            ps_pivot = ps_ratings.groupBy(\"user\").pivot(\"item\").sum(\"rating\")\\\n",
    "                .withColumn('midx', F.monotonically_increasing_id()).select(\"*\")\\\n",
    "                .withColumn('idx', F.row_number().over(wd) - 1)\\\n",
    "                .drop('midx').checkpoint()\n",
    "        \n",
    "        # recommend\n",
    "        print('Start loop predict rating ', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        total = ps_pivot.count()\n",
    "        step = 5000\n",
    "        for i in range(0, total, step):\n",
    "            print(i, \"/\", total)\n",
    "\n",
    "            # Get chunk of data to do similarities\n",
    "            pst = ps_pivot.filter((F.col('idx') >= i) & (F.col('idx') < i+step)).select('*')\n",
    "\n",
    "            sims_rating = sims.join(pst.select('user'), (F.col('id_1') == F.col('user')) | (F.col('id_2') == F.col('user')), \"inner\")\\\n",
    "                            .filter(~F.col(\"user\").isNull())\\\n",
    "                            .withColumn('user_sim', \\\n",
    "                                        F.when(F.col('id_1') == F.col('user'), F.col('id_2')).otherwise(F.col('id_1')))\\\n",
    "                            .select('user', 'user_sim', 'sim').alias('sm')\\\n",
    "                            .join(ps_pivot.alias('pv'), F.col('sm.user_sim') == F.col('pv.user'), \"inner\")\\\n",
    "                            .select(['sm.user', 'sm.user_sim', 'sm.sim']\\\n",
    "                                        + [F.col('pv.' + c) for c in ps_pivot.columns if c not in ['user']])\n",
    "\n",
    "            # schema\n",
    "            pstt = self.__predict_rating(sims_rating, 'user')\n",
    "            ps_ret = pstt if ps_ret is None else ps_ret.union(pstt)\n",
    "        \n",
    "        print('End loop predict rating ', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        ps_ret = self.__recommend_sub(ps_ret, top_product)\n",
    "        \n",
    "        return ps_ret, ps_pivot\n",
    "\n",
    "    def __recommend_sub(self, ps_ret, top_product):\n",
    "        import pyspark.sql.functions as F\n",
    "        import numpy as np\n",
    "        from numpy.linalg import norm\n",
    "        from pyspark.sql.types import IntegerType, DoubleType, StructField, StringType, FloatType, StructType\n",
    "        from pyspark.sql.window import Window\n",
    "        from pyspark.ml.linalg import Vectors\n",
    "        from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "        import gc\n",
    "        \n",
    "        ps_ratings = self.Ybar_data\n",
    "        \n",
    "        # Exclude existing ratings from ps_ratings\n",
    "        ps_ret = ps_ret.alias(\"a\").join(ps_ratings.select('user', 'item', 'rating').alias(\"b\"), \n",
    "                   (ps_ret.user_id == ps_ratings.user) & (ps_ret.item_id == ps_ratings.item), \"left\")\\\n",
    "                    .filter(F.col(\"rating\").isNull())\\\n",
    "                    .select(F.col(\"a.user_id\"), F.col(\"a.item_id\"), F.col(\"a.predict_rating\"))\n",
    "\n",
    "        # Top product\n",
    "        if top_product is not None:\n",
    "            windowDept = Window.partitionBy(\"user_id\").orderBy(F.col(\"predict_rating\").desc())\n",
    "            ps_ret = ps_ret.withColumn(\"top\", \\\n",
    "                                F.row_number().over(windowDept)).filter(F.col(\"top\") <= top_product) \\\n",
    "                                .select(\"user_id\", \"item_id\", \"predict_rating\")\n",
    "            \n",
    "        return ps_ret\n",
    "    \n",
    "    def __recommend_all(self, top_product=None, db_write=None, file_recommend=None):\n",
    "        import pyspark.sql.functions as F\n",
    "        import numpy as np\n",
    "        from numpy.linalg import norm\n",
    "        from pyspark.sql.types import IntegerType, DoubleType, StructField, StringType, FloatType, StructType\n",
    "        from pyspark.sql.window import Window\n",
    "        from pyspark.ml.linalg import Vectors\n",
    "        from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "        import gc\n",
    "        \n",
    "        ps_ratings = self.Ybar_data\n",
    "        ps_ret_all, ps_pivot = self.recommend_all_sim_users(top_product)\n",
    "        ps_ret_sim, _ = self.recommend_all_users(top_product, ps_pivot)\n",
    "        ps_ret = ps_ret_all.union(ps_ret_sim).dropDuplicates()\n",
    "        \n",
    "        from datetime import datetime\n",
    "        ps_ret = ps_ret.withColumn('par_col', F.lit(datetime.now().strftime(\"%Y-%m-%d\")))\n",
    "        \n",
    "        if ps_ret.count() > 0:\n",
    "            if file_recommend is not None:\n",
    "                ps_ret.write.mode(\"append\").partitionBy('par_col').parquet(file_recommend)\n",
    "                \n",
    "            if db_write is not None:                \n",
    "                ps_ret.write.mode(\"overwrite\") \\\n",
    "                    .format(\"jdbc\") \\\n",
    "                    .option(\"url\", f\"jdbc:sqlserver://{db_write['host']};databaseName={db_write['database']}\") \\\n",
    "                    .option(\"dbtable\", db_write['table']) \\\n",
    "                    .option(\"user\", db_write['username']) \\\n",
    "                    .option(\"password\", db_write['password']) \\\n",
    "                    .save()\n",
    "        else:\n",
    "            print('No existing record to write into DB / HDFS')\n",
    "                          \n",
    "        return ps_ret\n",
    "\n",
    "    \n",
    "    def recommend_all_sim_users(self, top_product=None, ps_pivot=None):\n",
    "        import pyspark.sql.functions as F\n",
    "        import numpy as np\n",
    "        from numpy.linalg import norm\n",
    "        from pyspark.sql.types import IntegerType, DoubleType, StructField, StringType, FloatType, StructType\n",
    "        from pyspark.sql.window import Window\n",
    "        from pyspark.ml.linalg import Vectors\n",
    "        from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "        import gc\n",
    "        from itertools import chain\n",
    "        \n",
    "        sims = self.S\n",
    "        ps_ratings = self.Ybar_data\n",
    "        ps_ret = None\n",
    "        \n",
    "        # pivot ratings\n",
    "        if ps_pivot is None:\n",
    "            print('Building pivot ', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "            wd = Window.orderBy('midx')\n",
    "            ps_pivot = ps_ratings.groupBy(\"user\").pivot(\"item\").sum(\"rating\")\\\n",
    "                .withColumn('midx', F.monotonically_increasing_id()).select(\"*\")\\\n",
    "                .withColumn('idx', F.row_number().over(wd) - 1)\\\n",
    "                .drop('midx').checkpoint()\n",
    "        \n",
    "        # Get sims join rating\n",
    "        print('Get similarity list', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        sims_rating = sims.join(ps_pivot.select('user'), (F.col('id_1') == F.col('user')) | (F.col('id_2') == F.col('user')), \"inner\")\\\n",
    "                            .filter(~F.col(\"user\").isNull())\\\n",
    "                            .withColumn('user_sim', \\\n",
    "                                        F.when(F.col('id_1') == F.col('user'), F.col('id_2')).otherwise(F.col('id_1')))\\\n",
    "                            .select('user', 'user_sim', 'sim')\\\n",
    "                            .groupby('user_sim').agg(F.collect_list(\"user\"))\n",
    "        sims_rating.cache()\n",
    "        sims_rating.collect()\n",
    "\n",
    "        # \n",
    "        print('Start loop predict rating ', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        df_sims_rating = sims_rating.to_pandas_on_spark()\n",
    "        step = 5000\n",
    "        total = len(df_sims_rating)\n",
    "        for i in range(0, total, step):\n",
    "            dft = df_sims_rating.iloc[i:i+step, :]\n",
    "            ls_id_s = list(dft.iloc[:, 0].values)\n",
    "            ls_id = list(set(chain(*dft.iloc[:, 1].values)))\n",
    "\n",
    "            sims_spl = sims.filter(F.col('id_1').isin(ls_id_s) | F.col('id_2').isin(ls_id_s)).select('*')\n",
    "            pivot_spl = ps_pivot.filter(F.col('user').isin(ls_id))\n",
    "\n",
    "            sims_rating = sims_spl.join(pivot_spl.select('user'), (F.col('id_1') == F.col('user')) | (F.col('id_2') == F.col('user')), \"inner\")\\\n",
    "                            .filter(~F.col(\"user\").isNull())\\\n",
    "                            .withColumn('user_sim', \\\n",
    "                                        F.when(F.col('id_1') == F.col('user'), F.col('id_2')).otherwise(F.col('id_1')))\\\n",
    "                            .select('user', 'user_sim', 'sim').alias('sm')\\\n",
    "                            .join(pivot_spl.alias('pv'), F.col('sm.user_sim') == F.col('pv.user'), \"inner\")\\\n",
    "                            .select(['sm.user', 'sm.user_sim', 'sm.sim']\\\n",
    "                                        + [F.col('pv.' + c) for c in pivot_spl.columns if c not in ['user']])\n",
    "\n",
    "            pst = self.__predict_rating(sims_rating, 'user_sim')\n",
    "            ps_ret = pst if ps_ret is None else ps_ret.union(pst)\n",
    "        \n",
    "        print('End loop predict rating ', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        ps_ret = self.__recommend_sub(ps_ret, top_product)\n",
    "            \n",
    "        return ps_ret, ps_pivot\n",
    "    \n",
    "    def __predict_rating(self, sims_rating, gb_col):\n",
    "        import pyspark.sql.functions as F\n",
    "        import numpy as np\n",
    "        from numpy.linalg import norm\n",
    "        from pyspark.sql.types import IntegerType, DoubleType, StructField, StringType, FloatType, StructType\n",
    "        from pyspark.sql.window import Window\n",
    "        from pyspark.ml.linalg import Vectors\n",
    "        from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "        import gc\n",
    "        \n",
    "        schema = StructType([StructField('user_id', StringType()),\n",
    "                         StructField('item_id', StringType()),\n",
    "                        StructField('predict_rating', DoubleType())])\n",
    "        k_nearest = self.k_nearest\n",
    "\n",
    "        def sim_udf(df):\n",
    "            ret = []\n",
    "            user_id = df[gb_col].iloc[0]\n",
    "\n",
    "            # Get nearest sim\n",
    "            cols = list(set(df.columns) - set(['user_sim', 'sim', 'idx', 'user']))\n",
    "            for c in cols:\n",
    "                dft = df[df[c].notnull()].sort_values(by='sim', ascending=False).head(k_nearest)\n",
    "\n",
    "                if len(dft) > 0:        \n",
    "                    r = np.array(list(df[c]))\n",
    "                    s = np.array(list(df['sim']))\n",
    "                    x_rate = (r*s).sum()/(np.abs(s).sum() + 1e-8)\n",
    "                    ret.append([user_id] + [c] + [x_rate])\n",
    "            return pd.DataFrame(ret)\n",
    "\n",
    "\n",
    "        return sims_rating.groupBy(gb_col).applyInPandas(sim_udf, schema)\\\n",
    "                    .filter(~F.col(\"predict_rating\").isNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abea0e6-bb6b-40ef-8136-f148815f3117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947615ee-8681-4f41-ad8a-fe182e6ab827",
   "metadata": {},
   "outputs": [],
   "source": [
    "METABASE_SSH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d08e4a5c-f330-4158-b9fe-c6da24c0e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FILE_COMMON_PATH + \"lib_common.ipynb\"\n",
    "%run $f\n",
    "\n",
    "f = FILE_COMMON_PATH + \"lib_db.ipynb\"\n",
    "%run $f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a8ca9-13c5-4f79-814e-7df4f8b9a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FILE_COMMON_PATH + \"config.ipynb\"\n",
    "%run $f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d8f982-d8cf-4d1e-b9ff-8710082fc0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FILE_COMMON_PATH + \"lib_ml.ipynb\"\n",
    "%run $f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45a1f676-19ef-45ce-9d75-4c829b14c49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drivername': 'mssql+pyodbc',\n",
       " 'username': 'administrator2019',\n",
       " 'password': 'paysmart@2019',\n",
       " 'host': 'paysmart-svr.database.windows.net',\n",
       " 'port': 1433,\n",
       " 'database': 'paysmart_dwh',\n",
       " 'driver': 'ODBC Driver 18 for SQL Server'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AZURE_DB['schema'] = 'dds'\n",
    "\n",
    "DB_WRITE = AZURE_DB.copy()\n",
    "TABLE_WRITE = 'recommendation'\n",
    "DB_WRITE['schema'] = 'dds'\n",
    "DB_WRITE['table'] = TABLE_WRITE\n",
    "\n",
    "DB_TEMP = DB_WRITE.copy()\n",
    "TABLE_TEMP = TABLE_WRITE + \"_tmp\"\n",
    "DB_TEMP['schema'] = 'history'\n",
    "DB_TEMP['table'] = TABLE_TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ae9fe32-40a7-4307-8c4b-daceb8f5df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trans_group(spark_ss, group_field_1, group_field_2, calculate_field, calculate_func, from_date):\n",
    "    \n",
    "    hdfs_trans = \"hdfs://master:9000/all_data/raw/az_transaction.parquet\" \n",
    "    sdf_txn = spark_ss.read.parquet(hdfs_trans) \n",
    "    sdf_txn = sdf_txn.filter(sdf_txn.txn_status == 1)\n",
    "    sdf_txn = sdf_txn.filter(sdf_txn.txn_date > from_date)\n",
    "    \n",
    "    if calculate_func == 'count':\n",
    "        dft = sdf_txn.groupBy(group_field_1, group_field_2).count()\n",
    "        return dft.withColumnRenamed(group_field_2, 'group_field').withColumnRenamed('count', 'cal_field')\n",
    "    \n",
    "    elif calculate_func == 'sum':\n",
    "        dft = sdf_txn.groupBy(group_field_1, group_field_2).sum(calculate_field)\n",
    "        return dft.withColumnRenamed(group_field_2, 'group_field').withColumnRenamed('sum', 'cal_field')\n",
    "    \n",
    "    return dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "292340e0-c4c7-4c2e-96b2-46c6fefc24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from threading import Thread\n",
    "\n",
    "class ThreadWithReturnValue(Thread):\n",
    "    \n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs={}, Verbose=None):\n",
    "        Thread.__init__(self, group, target, name, args, kwargs)\n",
    "        self._return = None\n",
    "\n",
    "    def run(self):\n",
    "        if self._target is not None:\n",
    "            self._return = self._target(*self._args,\n",
    "                                                **self._kwargs)\n",
    "    def join(self, *args):\n",
    "        Thread.join(self, *args)\n",
    "        return self._return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fbe120-ac6d-4a58-87b1-a371f3f7496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "running_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "trans_from_date = (datetime.now() - timedelta(days=14)).strftime(\"%Y-%m-%d\")\n",
    "file_sim = \"hdfs://master:9000/all_data/features/recommend_similarity.parquet\"\n",
    "file_sim_history = \"hdfs://master:9000/all_data/features/recommend_sim_hist.parquet\"\n",
    "running_date, trans_from_date, file_sim, file_sim_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf74888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running all\n",
    "from datetime import datetime, timedelta\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([('spark.master', 'spark://master:7077'),\n",
    "                                    ('spark.executor.memory', '2g'), \n",
    "                                  ('spark.executor.cores', '2'), \n",
    "                                  ('spark.cores.max', '2'), \n",
    "                                  ('spark.driver.memory','10g')])\n",
    "\n",
    "# This is to fix \"writing dates before 1582-10-15 or timestamps before into Parquet INT96\"\n",
    "conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n",
    "conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
    "conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "conf.set('spark.driver.extraClassPath', '/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar')\n",
    "conf.set('spark.executor.extraClassPath', '/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar')\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de5e7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "spark_ss = SparkSession.builder.appName('recommendation_similarity').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed7e2688-83dc-4b59-b335-e2ffd6ef9b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_group_field_2 = ['txn_type', 'service_code', 'bank_code', 'segment_1', 'segment_2', 'order_service', \n",
    "                'service_provider_code', 'order_info_partner_code', 'service_package_category', \n",
    "               'service_package_subcategory']\n",
    "group_field_1 = \"wallet_id\"\n",
    "calculate_field = \"txn_amount\"\n",
    "calculate_func = \"count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa716cd1-faab-46c1-90ab-70d158c25b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all data\n",
    "print('Get all data', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "all_threads = [ThreadWithReturnValue(target=get_trans_group, \n",
    "                                args=(spark_ss, group_field_1, f2, calculate_field, calculate_func, trans_from_date)) \n",
    "    for f2 in ls_group_field_2]\n",
    "[t.start() for t in all_threads]\n",
    "ls_df = [t.join() for t in all_threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71582fd0-a74e-45db-a6f6-27cb5da53275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import percent_rank, concat, lit\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Merge data\n",
    "df_all, items_id, users_id = None, [], []\n",
    "windowSpec  = Window.partitionBy(\"group_field\")\n",
    "\n",
    "for i, dft in enumerate(ls_df):\n",
    "    dft = dft.filter(dft.group_field.isNotNull()).filter(dft.group_field != \"None\")\\\n",
    "        .withColumn(\"group_field\", concat(lit(ls_group_field_2[i]), lit(\"_\"), \"group_field\"))\\\n",
    "        .withColumn('maxi', F.max(\"cal_field\").over(windowSpec))\\\n",
    "        .withColumn('mini', F.min(\"cal_field\").over(windowSpec))\\\n",
    "        .withColumn(\"rating\", F.round(99 * ((F.col(\"cal_field\") - F.col('mini')) / (F.col('maxi') - F.col('mini'))) + 1, 2))\\\n",
    "        .drop('mini')\\\n",
    "        .drop('maxi')\n",
    "    df_all = dft if df_all is None else df_all.union(dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232be9c2-9f28-4dd5-991c-107699a98011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.withColumn(\"user\", df_all.wallet_id)\n",
    "df_all = df_all.withColumn(\"item\", df_all.group_field)\n",
    "df_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a85f10-34c4-4eca-8a9e-17e6ff18adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir(\"hdfs://master:9000/rdd_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c5326-7d7b-431b-bce2-cd0353d60203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save similarity file\n",
    "cf_rec = CF(df_all, 3, file_sim)\n",
    "cf_rec.load_sim()\n",
    "cf_rec.normalize_Y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05463efc-e66b-4ee2-a15f-cceec92f5e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recommend all\n",
    "# ps_new_rating = cf_rec.recommend_all(3)\n",
    "# ps_new_rating.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd6bde-2b40-441d-94db-a80c3429aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend full\n",
    "ps_new_rating = cf_rec.recommend_all(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56feaf7d-d360-41ef-8a63-77343cea7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_rating(ps_new_rating, file_recommend, METABASE_SSH, DB_WRITE, DB_TEMP):\n",
    "    if ps_new_rating.count() > 0:\n",
    "    \n",
    "        # Save to HDFS\n",
    "        print('Save to HDFS')\n",
    "        ps_new_rating.write.mode(\"append\").partitionBy('par_col').parquet(file_recommend)\n",
    "\n",
    "        # Save to DB\n",
    "        print('Save to DB')\n",
    "        ps_new_rating.write.mode(\"overwrite\") \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", f\"jdbc:sqlserver://{DB_TEMP['host']};databaseName={DB_TEMP['database']}\") \\\n",
    "                .option(\"dbtable\", f\"{DB_TEMP['schema']}.{DB_TEMP['table']}\") \\\n",
    "                .option(\"user\", DB_TEMP['username']) \\\n",
    "                .option(\"password\", DB_TEMP['password']) \\\n",
    "                .save()\n",
    "        upsert_sql(METABASE_SSH, DB_WRITE, DB_TEMP, {})    \n",
    "    else:\n",
    "        print('No existing record to write into DB / HDFS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c3666-62b4-43d1-a584-afbdaeeeb0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "top_product = 5\n",
    "top_user = 100\n",
    "\n",
    "if top_product is not None:\n",
    "    windowDept = Window.partitionBy(\"user_id\").orderBy(F.col(\"predict_rating\").desc())\n",
    "    ps_new_rating_u = ps_new_rating.withColumn(\"top\", \\\n",
    "                        F.row_number().over(windowDept)).filter(F.col(\"top\") <= top_product) \\\n",
    "                        .select(\"user_id\", \"item_id\", \"predict_rating\")\\\n",
    "                        .withColumn('par_col', F.lit(datetime.now().strftime(\"%Y-%m-%d\")))        \n",
    "    file_recommend = \"hdfs://master:9000/all_data/features/recommendation.parquet\"\n",
    "    save_new_rating(ps_new_rating_u, file_recommend, METABASE_SSH, DB_WRITE, DB_TEMP)\n",
    "    \n",
    "    \n",
    "    \n",
    "if top_user is not None:\n",
    "    windowDept = Window.partitionBy(\"item_id\").orderBy(F.col(\"predict_rating\").desc())\n",
    "    ps_new_rating_i = ps_new_rating.withColumn(\"top\", \\\n",
    "                        F.row_number().over(windowDept)).filter(F.col(\"top\") <= top_user) \\\n",
    "                        .select(\"user_id\", \"item_id\", \"predict_rating\")\\\n",
    "                        .withColumn('par_col', F.lit(datetime.now().strftime(\"%Y-%m-%d\")))\n",
    "    file_recommend = \"hdfs://master:9000/all_data/features/recommendation_gb_item.parquet\"\n",
    "    DB_WRITE['table'] = 'recommendation_gb_item'\n",
    "    DB_TEMP['table'] = DB_WRITE['table'] + \"_tmp\"\n",
    "    save_new_rating(ps_new_rating_i, file_recommend, METABASE_SSH, DB_WRITE, DB_TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b5aba7-3908-4eea-8131-1e3821b2170f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54935dd5-2980-486d-82a8-a6398f284fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
